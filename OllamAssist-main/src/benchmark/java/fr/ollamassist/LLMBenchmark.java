package fr.ollamassist;

import dev.langchain4j.memory.chat.MessageWindowChatMemory;
import dev.langchain4j.model.ollama.OllamaChatModel;
import dev.langchain4j.model.ollama.OllamaStreamingChatModel;
import dev.langchain4j.rag.content.retriever.EmbeddingStoreContentRetriever;
import dev.langchain4j.service.AiServices;
import fr.baretto.ollamassist.chat.rag.DocumentIngestFactory;
import fr.baretto.ollamassist.chat.service.Assistant;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


class LLMBenchmark {

    private static final Logger logger = LoggerFactory.getLogger(LLMBenchmark.class);
    private static final String OLLAMA_URL = "http://localhost:11434";

    private LLMEvaluator evaluator;
    private Assistant assistant;

    public LLMBenchmark() {
        OllamaChatModel model = OllamaChatModel.builder()
                .baseUrl(OLLAMA_URL)
                .modelName("mistral:7b")
                .build();

        evaluator = AiServices.builder(LLMEvaluator.class)
                .chatModel(model)
                .build();


        OllamaStreamingChatModel chatModel = OllamaStreamingChatModel.builder()
                .temperature(0.2)
                .topK(70)
                .baseUrl(OLLAMA_URL)
                .modelName("llama3.1:7b")
                .build();


        // LuceneEmbeddingStore embeddingStore = new LuceneEmbeddingStore(project);

        assistant = AiServices.builder(Assistant.class)
                .streamingChatModel(chatModel)
                .chatMemory(MessageWindowChatMemory.withMaxMessages(15))
                .contentRetriever(EmbeddingStoreContentRetriever
                        .builder()
                        .embeddingModel(DocumentIngestFactory.createEmbeddingModel())
                        .dynamicMaxResults(query -> 3)
                        .dynamicMinScore(query -> 0.85)
                        //  .embeddingStore(embeddingStore)
                        .build())
                .build();
    }

    private void evaluate(String question, String response, String expected, String sources) {
        String prompt = buildPrompt(question, response, expected, sources);
        String judgment = evaluator.benchmark(prompt);
        logger.info(judgment);
    }

    private String buildPrompt(String question, String response, String expected, String sources) {
        return """
                You are a strict evaluator. Given the following elements:
                
                Question: %s
                
                Answer generated by the chatbot:
                %s
                
                Expected answer:
                %s
                
                Retrieved documents:
                %s
                
                Please rate the generated answer on:
                - Relevance (does it answer the question?)
                - Faithfulness (is it grounded in the documents?)
                - Completeness (does it miss important elements?)
                
                Respond with a score from 1 to 10 and a short explanation.
                """.formatted(question, response, expected, sources);
    }

    private interface LLMEvaluator {
        String benchmark(String prompt);
    }


    void benchmark_chat_response() {

    }


    public static void main(String[] args) {
        LLMBenchmark benchmark = new LLMBenchmark();
        benchmark.benchmark_chat_response();
    }
}
